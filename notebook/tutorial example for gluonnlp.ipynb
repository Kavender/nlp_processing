{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-14T23:23:16.831204Z",
     "start_time": "2020-02-14T23:23:12.768995Z"
    }
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import mxnet as mx\n",
    "from mxnet import gluon, nd\n",
    "import gluonnlp as nlp\n",
    "import gluonnlp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:10:13.997867Z",
     "start_time": "2020-02-15T04:10:13.994585Z"
    }
   },
   "outputs": [],
   "source": [
    "RE_HYPHENATED_WORD = re.compile(\n",
    "    r\"(\\w{2,}(?<!\\d))\\s+-\\s+((?!\\d)\\w{2,})\",\n",
    "    flags=re.UNICODE | re.IGNORECASE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:10:14.432418Z",
     "start_time": "2020-02-15T04:10:14.429516Z"
    }
   },
   "outputs": [],
   "source": [
    "def normalize_hyphenated_words(text):\n",
    "    \"\"\"Normalize words that have been split by a hyphen by joining the pieces back together.\"\"\"\n",
    "    return RE_HYPHENATED_WORD.sub(r\"\\1\\2\", text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-02-15T04:10:15.686197Z",
     "start_time": "2020-02-15T04:10:15.681145Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ridesharing'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "normalize_hyphenated_words(\"ride - sharing\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:05.663381Z",
     "start_time": "2019-11-19T04:05:05.290515Z"
    }
   },
   "outputs": [],
   "source": [
    "train_dataset, test_dataset = [nlp.data.IMDB(root='data/imdb', segment=segment) for segment in ('train', 'test')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:05.669391Z",
     "start_time": "2019-11-19T04:05:05.665749Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#training samples=25000, #testing samples=25000\n"
     ]
    }
   ],
   "source": [
    "print('#training samples={:d}, #testing samples={:d}'.format(len(train_dataset), len(test_dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:05.994724Z",
     "start_time": "2019-11-19T04:05:05.991654Z"
    }
   },
   "outputs": [],
   "source": [
    "sentence = 'This is a very awesome, life-changing sentence.'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:07.596692Z",
     "start_time": "2019-11-19T04:05:06.455834Z"
    }
   },
   "outputs": [],
   "source": [
    "tokenizer = gluonnlp.data.SpacyTokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:08.320106Z",
     "start_time": "2019-11-19T04:05:07.598993Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading test-0690baed.bpe from http://repo.mxnet.io/gluon/dataset/vocab/test-0690baed.bpe...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'This is a very awesome, life-changing sentence.'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "url = 'http://repo.mxnet.io/gluon/dataset/vocab/test-0690baed.bpe'\n",
    "f = gluon.utils.download(url, overwrite=True)\n",
    "tokenizer = gluonnlp.data.SentencepieceTokenizer(f)\n",
    "detokenizer = gluonnlp.data.SentencepieceDetokenizer(f)\n",
    "tokenizer(sentence)\n",
    "detokenizer(tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:08.338139Z",
     "start_time": "2019-11-19T04:05:08.323160Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'very', 'awesome', ',', 'life', '-', 'changing', 'sentence', '.']\n",
      "['This', 'is', 'a', 'very', 'awesome', ',', 'life', '-', 'changing', 'sentence', '.']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = gluonnlp.data.BERTBasicTokenizer(lower=True)\n",
    "print(tokenizer(sentence))\n",
    "tokenizer = gluonnlp.data.BERTBasicTokenizer(lower=False)\n",
    "print(tokenizer(sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:09.215675Z",
     "start_time": "2019-11-19T04:05:08.341316Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading ./datasets/subj/all-9e7bd1da.zip from https://apache-mxnet.s3-accelerate.dualstack.amazonaws.com/gluon/dataset/subj/all-9e7bd1da.zip...\n"
     ]
    }
   ],
   "source": [
    "subj = gluonnlp.data.SUBJ(root='./datasets/subj')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:05:09.812564Z",
     "start_time": "2019-11-19T04:05:09.218296Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['gl', '##uo', '##nn', '##lp', ':', '使', 'nl', '##p', '变', '得', '简', '单', '。']\n"
     ]
    }
   ],
   "source": [
    "_, vocab = gluonnlp.model.bert_12_768_12(dataset_name='wiki_multilingual_uncased',\n",
    "                                       pretrained=False, root='./model')\n",
    "\n",
    "tokenizer = gluonnlp.data.BERTTokenizer(vocab=vocab)\n",
    "print(tokenizer('gluonnlp: 使NLP变得简单。'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:36:01.015099Z",
     "start_time": "2019-11-19T04:36:01.012061Z"
    }
   },
   "outputs": [],
   "source": [
    "sentences = [\n",
    "    \"作为语言而言，为世界使用人数最多的语言，目前世界有五分之一人口做为母语。\",\n",
    "    \"汉语有多种分支，当中官话最为流行，为中华人民共和国的国家通用语言（又称为普通话）、以及中华民国的国语。\",\n",
    "    \"此外，中文还是联合国正式语文，并被上海合作组织等国际组织采用为官方语言。\",\n",
    "    \"在中国大陆，汉语通称为“汉语”。\",\n",
    "    \"在联合国、台湾、香港及澳门，通称为“中文”。\",\n",
    "    \"在新加坡及马来西亚，通称为“华语”。\",\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:09:07.944364Z",
     "start_time": "2019-11-19T04:09:07.155064Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /var/folders/km/wk9tg7bd6490br6rp1sz_y100000gn/T/jieba.cache\n",
      "Loading model cost 0.777 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['我', '来到', '北京', '清华大学']\n",
      "['小明', '硕士', '毕业', '于', '中国科学院', '计算所', '，', '后', '在', '日本京都大学', '深造']\n"
     ]
    }
   ],
   "source": [
    "tokenizer = gluonnlp.data.JiebaTokenizer()\n",
    "print(tokenizer('我来到北京清华大学'))\n",
    "print(tokenizer('小明硕士毕业于中国科学院计算所，后在日本京都大学深造'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:37:10.608866Z",
     "start_time": "2019-11-19T04:37:10.606272Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T04:42:13.365432Z",
     "start_time": "2019-11-19T04:42:13.347857Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[小明, 硕士, 毕业, 于, 中国科学院, 计算所, ，, 后, 在, 日本京都大学, 深造]\n"
     ]
    }
   ],
   "source": [
    "from spacy.lang.zh import Chinese\n",
    "\n",
    "nlp = Chinese()\n",
    "doc = nlp(u\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")\n",
    "print([tok for tok in doc])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T13:47:20.033823Z",
     "start_time": "2019-11-19T13:47:20.029498Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spacy.tokens.doc.Doc"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nlp.tokenizer(u\"小明硕士毕业于中国科学院计算所，后在日本京都大学深造\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-19T14:01:26.232865Z",
     "start_time": "2019-11-19T14:01:26.212521Z"
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-20T02:43:53.986297Z",
     "start_time": "2019-11-20T02:43:53.889241Z"
    }
   },
   "outputs": [],
   "source": [
    "from mosestokenizer import MosesTokenizer, MosesDetokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T13:48:53.396454Z",
     "start_time": "2019-11-24T13:48:53.030412Z"
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import botocore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-11-24T14:10:46.943308Z",
     "start_time": "2019-11-24T14:10:46.933589Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "BUCKET_NAME = 'my-bucket' # replace with your bucket name\n",
    "KEY = 'my_image_in_s3.jpg' # replace with your object key\n",
    "\n",
    "\n",
    "# s3 = boto3.resource('s3')\n",
    "\n",
    "# try:\n",
    "#     s3.Bucket(BUCKET_NAME).download_file(KEY, 'my_local_image.jpg')\n",
    "# except botocore.exceptions.ClientError as e:\n",
    "#     if e.response['Error']['Code'] == \"404\":\n",
    "#         print(\"The object does not exist.\")\n",
    "#     else:\n",
    "#         raise\n",
    "\n",
    "def get_file_from_s3_path(s3_bucket: str, s3_key: str, local_dir: str, refresh: bool = False) -> bool:\n",
    "    download_path = os.path.join(local_dir, Path(s3_key).name)\n",
    "    path_exists = os.path.exists(download_path)\n",
    "    if not path_exists or refresh:\n",
    "        s3 = S3BucketReader(s3_bucket, s3_key)\n",
    "        s3.download_file(download_path)\n",
    "    if Path(s3_key).suffix == \".zip\":\n",
    "        with zipfile.ZipFile(download_path, \"r\") as zip_ref:\n",
    "            zip_ref.extractall(local_dir)\n",
    "        os.remove(download_path)\n",
    "\n",
    " \n",
    "    \n",
    "def download_file_with_client(access_key, secret_key, bucket_name, key, local_path):\n",
    "    client = boto3.client('s3',aws_access_key_id=access_key, aws_secret_access_key=secret_key)\n",
    "    client.download_file(bucket_name, key, local_path)\n",
    "    print('Downloaded frile with boto3 client')\n",
    "\n",
    "    \n",
    "    def download_file(self, file_path, extend_key: Optional[str] = None, raise_error=False) -> None:\n",
    "        key = _extend_key(self.base_key, extend_key)\n",
    "        client.download_file(Bucket=self.bucket_name, Key=key, Filename=file_path)\n",
    "\n",
    "\n",
    "# access_key = '<Access Key>'\n",
    "# secret_key = '<Secret Key>'\n",
    "# bucket_name = '<your bucket name>'\n",
    "# key = '<folder…/filename>'\n",
    "# local_path = '<e.g. ./log.txt>'\n",
    "settings = {\n",
    "            'name': 'my-files',\n",
    "            'protocol': 's3',\n",
    "            'location': 'bucket-name',\n",
    "            'readCredentials' : {\n",
    "                'aws_access_key_id': 'AAA',\n",
    "                'aws_secret_access_key': 'BBB'\n",
    "            }\n",
    "           }\n",
    "\n",
    "# download_file_with_client(access_key, secret_key, bucket_name, key, local_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def upload_file_to_s3(key_name, file_name, bucket_name, region_name, acl=None):\n",
    "    try:\n",
    "        s3_connection = boto3.resource(\"s3\", region_name=region_name)\n",
    "        data = open(file_name, \"rb\")\n",
    "        if acl:\n",
    "            response = s3_connection.Object(bucket_name, key_name).put(Body=data, ACL=acl)\n",
    "        else:\n",
    "            response = s3_connection.Object(bucket_name, key_name).put(Body=data)\n",
    "\n",
    "        status_code = response[\"ResponseMetadata\"][\"HTTPStatusCode\"]\n",
    "        return status_code == 200\n",
    "    except ClientError as error:\n",
    "        print(f\"Received error when uploading file to s3: {error}\")\n",
    "        \n",
    "    return False\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bert_experiment",
   "language": "python",
   "name": "bert_experiment"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
